{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import os\n",
    "import random\n",
    "import os\n",
    "from typing import List, Any, Tuple, Optional\n",
    "\n",
    "import PIL.Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import PIL\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import click\n",
    "import yaml\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from training_utils.train_loop import train_loop, TrainingConfig\n",
    "from generation import GenerationConfig\n",
    "from models.gen.blocks import BaseDiffusionModel, UNet, UNetConfig\n",
    "from models.gen.edm import EDM, EDMConfig\n",
    "from models.gen.ddpm import DDPM, DDPMConfig\n",
    "from utils.utils import EasyDict, instantiate_from_config\n",
    "from data.data import SequencesDataset\n",
    "\n",
    "def _save_sample_imgs(\n",
    "    frames_real: torch.Tensor,\n",
    "    frames_gen: List[torch.Tensor],\n",
    "    path: str\n",
    "):\n",
    "    height_row = 5\n",
    "    col_width = 5\n",
    "    cols = len(frames_real)\n",
    "    rows = 1 + len(frames_gen)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(col_width * cols, height_row * rows))\n",
    "    for row in range(rows):\n",
    "        frames = frames_real if row == 0 else frames_gen[row - 1]\n",
    "        for i in range(len(frames_real)):\n",
    "            axes[row, i].imshow(SequencesDataset.get_np_img(frames[i]))\n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "    plt.savefig(path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "def _generate_and_save_sample_imgs(\n",
    "    model: BaseDiffusionModel,\n",
    "    dataset: SequencesDataset,\n",
    "    epoch: int,\n",
    "    device: str,\n",
    "    context_length: int,\n",
    "    length_session = 20\n",
    "):\n",
    "    index = random.randint(0, len(dataset) - length_session - 1)\n",
    "    start, end = dataset.get_session(index, length_session)\n",
    "\n",
    "    data = dataset.dataset[start:end]\n",
    "\n",
    "    actions1 = data['first_0']\n",
    "    actions2 = data['second_0']\n",
    "    actions1 = torch.tensor(actions1)\n",
    "    actions2 = torch.tensor(actions2)\n",
    "    actions = torch.stack([actions1, actions2], dim=0).to(device)\n",
    "\n",
    "    # Denoise steps\n",
    "    gen_10_imgs = None\n",
    "    gen_5_imgs = None\n",
    "    gen_2_imgs = None\n",
    "    \n",
    "    for i in range(0, length_session- context_length):\n",
    "        if gen_10_imgs is None or gen_5_imgs is None or gen_2_imgs is None:\n",
    "            imgs = dataset.get_images(start, start + context_length)\n",
    "            imgs = torch.stack(imgs, dim=0).to(device)\n",
    "            \n",
    "            gen_10_imgs = imgs.clone()\n",
    "            gen_5_imgs = imgs.clone()\n",
    "            gen_2_imgs = imgs.clone()\n",
    "\n",
    "            img = imgs[-1]\n",
    "\n",
    "        prev_actions = actions[:, i: i + context_length].unsqueeze(0)\n",
    "        gen_img = model.sample(10, img.shape, gen_10_imgs[-context_length:].unsqueeze(0), prev_actions)[0]\n",
    "        gen_10_imgs = torch.concat([gen_10_imgs, gen_img[None, :, :, :]], dim=0)\n",
    "\n",
    "        gen_img = model.sample(5, img.shape, gen_5_imgs[-context_length:].unsqueeze(0), prev_actions)[0]\n",
    "        gen_5_imgs = torch.concat([gen_5_imgs, gen_img[None, :, :, :]], dim=0)\n",
    "\n",
    "        gen_img = model.sample(2, img.shape, gen_2_imgs[-context_length:].unsqueeze(0), prev_actions)[0]\n",
    "        gen_2_imgs = torch.concat([gen_2_imgs, gen_img[None, :, :, :]], dim=0)\n",
    "    \n",
    "    real_images = torch.stack(dataset.get_images(start, end), dim=0)\n",
    "    _save_sample_imgs(real_images, [gen_10_imgs, gen_5_imgs, gen_2_imgs], f\"val_images/{epoch}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_to_tensor = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((.5,.5,.5), (.5,.5,.5))\n",
    "])\n",
    "\n",
    "\n",
    "args = {\n",
    "    \"config\": \"../config/Diffusion.yaml\",\n",
    "    \"model_type\": \"ddpm\",\n",
    "    \"output_prefix\": f\"output/boxing\",\n",
    "    \"last_checkpoint\": \"\",\n",
    "    \"gen_val_images\": True,\n",
    "\n",
    "    # \"dataset\": \"data/sequences\",\n",
    "    # \"output_loader\": \"output/sequences_loader.pkl\",\n",
    "}\n",
    "options = EasyDict(**args)\n",
    "with open(options.config, 'r') as f:\n",
    "    config = EasyDict(**yaml.safe_load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Do not shuffle the dataset.\n",
    "dataset = load_dataset(\"betteracs/boxing_atari_diffusion\")\n",
    "\n",
    "train_seq_dataset = SequencesDataset(\n",
    "    dataset=dataset['train'].select(range(450_000)),\n",
    "    seq_length=config['generation']['context_length'],\n",
    "    transform=transform_to_tensor,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_seq_dataset = SequencesDataset(\n",
    "    dataset=dataset['train'].select(range(450_000, 500_000)),\n",
    "    seq_length=config['generation']['context_length'],\n",
    "    transform=transform_to_tensor,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_seq_dataset,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=config['training']['num_workers']\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_seq_dataset,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    num_workers=config['training']['num_workers']\n",
    ")\n",
    "\n",
    "training_config = TrainingConfig(**config.training)\n",
    "generation_config = GenerationConfig(**config.generation)\n",
    "\n",
    "# generation_config.image_size = 160\n",
    "# generation_config.context_length = 8\n",
    "# training_config.batch_size = 2\n",
    "\n",
    "transform_to_tensor = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((.5,.5,.5), (.5,.5,.5))\n",
    "])\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# For Mac OS\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "model: BaseDiffusionModel\n",
    "if options.model_type == \"edm\":\n",
    "    config = EDMConfig(**instantiate_from_config(config.edm))\n",
    "    model = EDM.from_config(\n",
    "        config=config,\n",
    "        context_length=generation_config.context_length,\n",
    "        device=device,\n",
    "        model=UNet.from_config(\n",
    "            config=config.unet,\n",
    "            in_channels=generation_config.unet_input_channels,\n",
    "            out_channels=generation_config.output_channels,\n",
    "            actions_count=generation_config.actions_count,\n",
    "            seq_length=generation_config.context_length\n",
    "        )\n",
    "    )\n",
    "elif options.model_type == \"ddpm\":\n",
    "    config = DDPMConfig(**instantiate_from_config(config.ddpm))\n",
    "    model = DDPM.from_config(\n",
    "        config=config,\n",
    "        context_length=generation_config.context_length,\n",
    "        device=device,\n",
    "        model=UNet.from_config(\n",
    "            config=config.unet,\n",
    "            in_channels=generation_config.unet_input_channels,\n",
    "            out_channels=generation_config.output_channels,\n",
    "            actions_count=generation_config.actions_count,\n",
    "            seq_length=generation_config.context_length,\n",
    "            T=config.T,\n",
    "            # player_autoencoder=config.player_autoencoder,\n",
    "        )\n",
    "    )\n",
    "\n",
    "def gen_val_images(epoch: int):\n",
    "    _generate_and_save_sample_imgs(model, val_seq_dataset, epoch, device, generation_config.context_length)\n",
    "\n",
    "\n",
    "print(f\"Start training {options.model_type}\")\n",
    "training_losses, val_losses = train_loop(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    config=training_config,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    output_path_prefix=options.output_prefix,\n",
    "    existing_model_path=None, #options[\"last_checkpoint\"],\n",
    "    gen_imgs=gen_val_images if options.gen_val_images else None\n",
    ")\n",
    "print(\"-\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-johnwick",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
