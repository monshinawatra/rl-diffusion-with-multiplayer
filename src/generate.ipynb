{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64e07337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoROM will download the Atari 2600 ROMs.\n",
      "They will be installed to:\n",
      "\t/home/monsh/anaconda3/envs/project-johnwick/lib/python3.11/site-packages/AutoROM/roms\n",
      "\t/home/monsh/anaconda3/envs/project-johnwick/lib/python3.11/site-packages/multi_agent_ale_py/roms\n",
      "\n",
      "Existing ROMs will be overwritten.\n"
     ]
    }
   ],
   "source": [
    "!AutoROM -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c9f730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.atari import boxing_v2\n",
    "import pygame\n",
    "import os\n",
    "\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b82f033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import supersuit as ss\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import CnnPolicy, MlpPolicy\n",
    "from pettingzoo.atari import boxing_v2\n",
    "\n",
    "\n",
    "\n",
    "def train(env_fn, steps: int = 10_000, seed: int | None = 0, **env_kwargs):\n",
    "    # Train a single model to play as each agent in an AEC environment\n",
    "    env = env_fn.parallel_env(**env_kwargs)\n",
    "\n",
    "    # Add black death wrapper so the number of agents stays constant\n",
    "    # MarkovVectorEnv does not support environments with varying numbers of active agents unless black_death is set to True\n",
    "\n",
    "    # Pre-process using SuperSuit\n",
    "    visual_observation = True #not env.unwrapped.vector_state\n",
    "    if visual_observation:\n",
    "        # If the observation space is visual, reduce the color channels, resize from 512px to 84px, and apply frame stacking\n",
    "        # env = ss.color_reduction_v0(env, mode=\"B\")\n",
    "        # env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "        env = ss.frame_stack_v1(env, 3)\n",
    "\n",
    "    env.reset(seed=seed)\n",
    "\n",
    "    print(f\"Starting training on {str(env.metadata['name'])}.\")\n",
    "\n",
    "    env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "    env = ss.concat_vec_envs_v1(env, 4, num_cpus=1, base_class=\"stable_baselines3\")\n",
    "\n",
    "    # Use a CNN policy if the observation space is visual\n",
    "    model = PPO(\n",
    "        CnnPolicy if visual_observation else MlpPolicy,\n",
    "        env,\n",
    "        verbose=1,\n",
    "        batch_size=64,\n",
    "        device=\"cuda\"\n",
    "    )\n",
    "\n",
    "    model.learn(total_timesteps=steps, progress_bar=True)\n",
    "\n",
    "    model.save(f\"{env.unwrapped.metadata.get('name')}_{time.strftime('%Y%m%d-%H%M%S')}\")\n",
    "\n",
    "    print(\"Model has been saved.\")\n",
    "\n",
    "    print(f\"Finished training on {str(env.unwrapped.metadata['name'])}.\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "def eval(env_fn, num_games: int = 100, render_mode: str | None = None, **env_kwargs):\n",
    "    # Evaluate a trained agent vs a random agent\n",
    "    env = env_fn.env(render_mode=render_mode, **env_kwargs)\n",
    "\n",
    "    # Pre-process using SuperSuit\n",
    "    visual_observation = True # not env.unwrapped.vector_state\n",
    "    if visual_observation:\n",
    "        # If the observation space is visual, reduce the color channels, resize from 512px to 84px, and apply frame stacking\n",
    "        env = ss.color_reduction_v0(env, mode=\"B\")\n",
    "        env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "        env = ss.frame_stack_v1(env, 3)\n",
    "\n",
    "    print(\n",
    "        f\"\\nStarting evaluation on {str(env.metadata['name'])} (num_games={num_games}, render_mode={render_mode})\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        latest_policy = max(\n",
    "            glob.glob(f\"{env.metadata['name']}*.zip\"), key=os.path.getctime\n",
    "        )\n",
    "    except ValueError:\n",
    "        print(\"Policy not found.\")\n",
    "        exit(0)\n",
    "\n",
    "    model = PPO.load(latest_policy)\n",
    "\n",
    "    rewards = {agent: 0 for agent in env.possible_agents}\n",
    "\n",
    "    # Note: we evaluate here using an AEC environments, to allow for easy A/B testing against random policies\n",
    "    # For example, we can see here that using a random agent for archer_0 results in less points than the trained agent\n",
    "    for i in range(num_games):\n",
    "        env.reset(seed=i)\n",
    "        env.action_space(env.possible_agents[0]).seed(i)\n",
    "\n",
    "        for agent in env.agent_iter():\n",
    "            obs, reward, termination, truncation, info = env.last()\n",
    "\n",
    "            for a in env.agents:\n",
    "                rewards[a] += env.rewards[a]\n",
    "\n",
    "            if termination or truncation:\n",
    "                break\n",
    "            else:\n",
    "                if agent == env.possible_agents[0]:\n",
    "                    act = env.action_space(agent).sample()\n",
    "                else:\n",
    "                    act = model.predict(obs, deterministic=True)[0]\n",
    "            env.step(act)\n",
    "    env.close()\n",
    "\n",
    "    avg_reward = sum(rewards.values()) / len(rewards.values())\n",
    "    avg_reward_per_agent = {\n",
    "        agent: rewards[agent] / num_games for agent in env.possible_agents\n",
    "    }\n",
    "    print(f\"Avg reward: {avg_reward}\")\n",
    "    print(\"Avg reward per agent, per game: \", avg_reward_per_agent)\n",
    "    print(\"Full rewards: \", rewards)\n",
    "\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58b22e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_fn = boxing_v2\n",
    "\n",
    "# # Set vector_state to false in order to use visual observations (significantly longer training time)\n",
    "# env_kwargs = {}\n",
    "\n",
    "# # Train a model (takes ~5 minutes on a laptop CPU)\n",
    "# train(env_fn, steps=819_200, seed=0, **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "211b1ea8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error: Unexpected observation shape (84, 84, 3) for Box environment, please use (8, 84, 84) or (n_env, 8, 84, 84) for the observation shape.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m         act \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space(agent)\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m         act \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# plt.imshow(env.render())\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n\u001b[1;32m     53\u001b[0m env\u001b[38;5;241m.\u001b[39mstep(act)\n",
      "File \u001b[0;32m/home/monsh/anaconda3/envs/project-johnwick/lib/python3.11/site-packages/stable_baselines3/common/base_class.py:557\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    539\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    542\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    543\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray, Optional[\u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[1;32m    544\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/monsh/anaconda3/envs/project-johnwick/lib/python3.11/site-packages/stable_baselines3/common/policies.py:365\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(observation) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    358\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have passed a tuple to the predict() function instead of a Numpy array or a Dict. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m     )\n\u001b[0;32m--> 365\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    368\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict(obs_tensor, deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n",
      "File \u001b[0;32m/home/monsh/anaconda3/envs/project-johnwick/lib/python3.11/site-packages/stable_baselines3/common/policies.py:272\u001b[0m, in \u001b[0;36mBaseModel.obs_to_tensor\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    268\u001b[0m     observation \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(observation)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;66;03m# Dict obs need to be handled separately\u001b[39;00m\n\u001b[0;32m--> 272\u001b[0m     vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[43mis_vectorized_observation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;66;03m# Add batch dimension if needed\u001b[39;00m\n\u001b[1;32m    274\u001b[0m     observation \u001b[38;5;241m=\u001b[39m observation\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape))  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "File \u001b[0;32m/home/monsh/anaconda3/envs/project-johnwick/lib/python3.11/site-packages/stable_baselines3/common/utils.py:402\u001b[0m, in \u001b[0;36mis_vectorized_observation\u001b[0;34m(observation, observation_space)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m space_type, is_vec_obs_func \u001b[38;5;129;01min\u001b[39;00m is_vec_obs_func_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation_space, space_type):\n\u001b[0;32m--> 402\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mis_vec_obs_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;66;03m# for-else happens if no break is called\u001b[39;00m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: Cannot determine if the observation is vectorized with the space type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/monsh/anaconda3/envs/project-johnwick/lib/python3.11/site-packages/stable_baselines3/common/utils.py:269\u001b[0m, in \u001b[0;36mis_vectorized_box_observation\u001b[0;34m(observation, observation_space)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: Unexpected observation shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBox environment, please use \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation_space\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor (n_env, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) for the observation shape.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, observation_space\u001b[38;5;241m.\u001b[39mshape)))\n\u001b[1;32m    273\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Error: Unexpected observation shape (84, 84, 3) for Box environment, please use (8, 84, 84) or (n_env, 8, 84, 84) for the observation shape."
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pettingzoo.atari import boxing_v2\n",
    "from gym.wrappers import RecordVideo\n",
    "\n",
    "render_mode = 'human'#\"rgb_array\" # \n",
    "num_games = 1\n",
    "\n",
    "# Evaluate a trained agent vs a random agent\n",
    "env = boxing_v2.env(render_mode=render_mode)\n",
    "\n",
    "# Pre-process using SuperSuit\n",
    "# visual_observation = not env.unwrapped.vector_state\n",
    "# if visual_observation:\n",
    "    # If the observation space is visual, reduce the color channels, resize from 512px to 84px, and apply frame stacking\n",
    "\n",
    "\n",
    "env = ss.color_reduction_v0(env, mode=\"B\")\n",
    "env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "env = ss.frame_stack_v1(env, 3)\n",
    "\n",
    "# env = RecordVideo(env=env, video_folder=\"video\", name_prefix=\"test-video\", episode_trigger=lambda x: x % 2 == 0)\n",
    "\n",
    "model = PPO.load(\"/home/monsh/works/image/boxing-engine/boxing_v2_20250305-130634.zip\")\n",
    "\n",
    "rewards = {agent: 0 for agent in env.possible_agents}\n",
    "\n",
    "seed = random.randint(0, 1000)\n",
    "\n",
    "# Note: we evaluate here using an AEC environments, to allow for easy A/B testing against random policies\n",
    "# For example, we can see here that using a random agent for archer_0 results in less points than the trained agent\n",
    "for i in range(num_games):\n",
    "    env.reset(seed=seed)\n",
    "    # env.start_video_recorder()\n",
    "    env.action_space(env.possible_agents[0]).seed(seed)\n",
    "\n",
    "    for agent in env.agent_iter():\n",
    "        obs, reward, termination, truncation, info = env.last()\n",
    "\n",
    "        for a in env.agents:\n",
    "            rewards[a] += env.rewards[a]\n",
    "\n",
    "        if termination or truncation:\n",
    "            break\n",
    "        else:\n",
    "            if agent in env.possible_agents[0]:\n",
    "                act = env.action_space(agent).sample()\n",
    "            else:\n",
    "                act = model.predict(obs, deterministic=True)[0]\n",
    "        \n",
    "        # plt.imshow(env.render())\n",
    "        # plt.show()\n",
    "\n",
    "        env.step(act)\n",
    "        # break\n",
    "    break\n",
    "\n",
    "# env.close_video_recorder()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e909075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0, 66],\n",
       "        [ 0,  0, 66],\n",
       "        [ 0,  0, 66]],\n",
       "\n",
       "       [[ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0, 66],\n",
       "        [ 0,  0, 66],\n",
       "        [ 0,  0, 66]],\n",
       "\n",
       "       [[ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0, 66],\n",
       "        [ 0,  0, 66],\n",
       "        [ 0,  0, 66]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0, 66],\n",
       "        [ 0,  0, 66],\n",
       "        [ 0,  0, 66]],\n",
       "\n",
       "       [[ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0, 66],\n",
       "        [ 0,  0, 66],\n",
       "        [ 0,  0, 66]],\n",
       "\n",
       "       [[ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0, 66],\n",
       "        [ 0,  0, 66],\n",
       "        [ 0,  0, 66]]], shape=(84, 84, 3), dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-johnwick",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
