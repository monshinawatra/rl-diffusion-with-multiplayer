{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import os\n",
    "import random\n",
    "import os\n",
    "from typing import List, Any, Tuple, Optional\n",
    "\n",
    "import PIL.Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import PIL\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import click\n",
    "import yaml\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from training_utils.train_loop import train_loop, TrainingConfig\n",
    "from generation import GenerationConfig\n",
    "from models.gen.blocks import BaseDiffusionModel, UNet, UNetConfig\n",
    "from models.gen.edm import EDM, EDMConfig\n",
    "from models.gen.ddpm import DDPM, DDPMConfig\n",
    "from utils.utils import EasyDict, instantiate_from_config\n",
    "from data.data import SequencesDataset\n",
    "\n",
    "def _save_sample_imgs(\n",
    "    frames_real: torch.Tensor,\n",
    "    frames_gen: List[torch.Tensor],\n",
    "    path: str\n",
    "):\n",
    "    def get_np_img(tensor: torch.Tensor) -> np.ndarray:\n",
    "        return (tensor * 127.5 + 127.5).long().clip(0,255).permute(1,2,0).detach().cpu().numpy().astype(np.uint8)\n",
    "\n",
    "    height_row = 5\n",
    "    col_width = 5\n",
    "    cols = len(frames_real)\n",
    "    rows = 1 + len(frames_gen)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(col_width * cols, height_row * rows))\n",
    "    for row in range(rows):\n",
    "        frames = frames_real if row == 0 else frames_gen[row - 1]\n",
    "        for i in range(len(frames_real)):\n",
    "            axes[row, i].imshow(get_np_img(frames[i]))\n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "    plt.savefig(path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "def _generate_and_save_sample_imgs(\n",
    "    model: BaseDiffusionModel,\n",
    "    dataset: SequencesDataset,\n",
    "    epoch: int,\n",
    "    device: str,\n",
    "    context_length: int,\n",
    "    length_session = 20\n",
    "):\n",
    "    if len(dataset) - 1 < length_session:\n",
    "        length_session = len(dataset) - 1\n",
    "    index = random.randint(0, len(dataset) - 1 - length_session)\n",
    "    print(dataset)\n",
    "    img, last_imgs, actions = dataset[index]\n",
    "\n",
    "    img = img.to(device)\n",
    "    last_imgs = last_imgs.to(device)\n",
    "    actions = actions.to(device)\n",
    "\n",
    "    real_imgs = last_imgs.clone()\n",
    "    gen_2_imgs = last_imgs.clone()\n",
    "    gen_10_imgs = last_imgs.clone()\n",
    "    gen_5_imgs = last_imgs.clone()\n",
    "    for j in range(1, length_session):\n",
    "        img, last_imgs, actions = dataset[index + j]\n",
    "        img = img.to(device)\n",
    "        last_imgs = last_imgs.to(device)\n",
    "        actions = actions.to(device)\n",
    "        real_imgs = torch.concat([real_imgs, img[None, :, :, :]], dim=0)\n",
    "\n",
    "        gen_img = model.sample(10, img.shape, gen_10_imgs[-context_length:].unsqueeze(0), actions.unsqueeze(0))[0]\n",
    "        gen_10_imgs = torch.concat([gen_10_imgs, gen_img[None, :, :, :]], dim=0)\n",
    "        gen_img = model.sample(2, img.shape, gen_2_imgs[-context_length:].unsqueeze(0), actions.unsqueeze(0))[0]\n",
    "        gen_2_imgs = torch.concat([gen_2_imgs, gen_img[None, :, :, :]], dim=0)\n",
    "        gen_img = model.sample(5, img.shape, gen_5_imgs[-context_length:].unsqueeze(0), actions.unsqueeze(0))[0]\n",
    "        gen_5_imgs = torch.concat([gen_5_imgs, gen_img[None, :, :, :]], dim=0)\n",
    "\n",
    "    _save_sample_imgs(real_imgs, [gen_10_imgs, gen_5_imgs, gen_2_imgs], f\"val_images/{epoch}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequencesDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        seq_length: int,\n",
    "        transform: Optional[Any] = None,\n",
    "        # one_player_possible_actions: int = 18\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.sequences: List[Tuple[List[int], List[int]]] = []\n",
    "        self.transform = transform\n",
    "        # self.one_player_possible_actions = one_player_possible_actions\n",
    "        \n",
    "        for i in tqdm(range(seq_length + 1, len(self.dataset), seq_length // 2)):\n",
    "            batches = self.dataset[max(i-seq_length - 1, 0) : i]\n",
    "            if batches['game_id'][0] == batches['game_id'][-1]:\n",
    "                self.sequences.append(((max(i-seq_length - 1, 0), i), [batch for batch in batches['first_0']], [batch for batch in batches['second_0']]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        imgs_range, actions1, actions2 = self.sequences[index]\n",
    "        imgs = [self.transform(self.dataset[i]['image']) for i in range(*imgs_range)]\n",
    "\n",
    "        last_img = imgs[-1]\n",
    "        # print(last_img.shape)\n",
    "        # actions = [\n",
    "        #     ac1 * self.one_player_possible_actions + ac2\n",
    "        #     for ac1, ac2 in zip(actions1, actions2)\n",
    "        # ]\n",
    "        actions1 = torch.tensor(actions1)[:-1]\n",
    "        actions2 = torch.tensor(actions2)[:-1]\n",
    "\n",
    "        return (last_img, torch.stack(imgs[:-1]), torch.stack([actions1, actions2], dim=0))\n",
    "\n",
    "\n",
    "def get_np_img(tensor: torch.Tensor) -> np.ndarray:\n",
    "    return (tensor * 127.5 + 127.5).long().clip(0,255).permute(1,2,0).detach().cpu().numpy().astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_to_tensor = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((.5,.5,.5), (.5,.5,.5))\n",
    "])\n",
    "dataset = load_dataset(\"betteracs/boxing_atari_diffusion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2498/2498 [00:05<00:00, 491.84it/s]\n"
     ]
    }
   ],
   "source": [
    "seq_dataset = SequencesDataset(\n",
    "    dataset=dataset['train'].select(range(0, 10000)),\n",
    "    seq_length=8,\n",
    "    transform=transform_to_tensor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = torch.utils.data.random_split(seq_dataset, [2400, 88])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Experiment6\n",
      "Start training ddpm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss for epoch 1: 0.4391:   7%|▋         | 41/600 [01:37<22:06,  2.37s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 128\u001b[0m\n\u001b[1;32m    124\u001b[0m         _generate_and_save_sample_imgs(model, val_set, epoch, device, generation_config\u001b[38;5;241m.\u001b[39mcontext_length)\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart training \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptions\u001b[38;5;241m.\u001b[39mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 128\u001b[0m     \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_path_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexisting_model_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#options[\"last_checkpoint\"],\u001b[39;49;00m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgen_imgs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgen_val_images\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen_val_images\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[0;32m/home/monsh/works/image/boxing/snake-diffusion/src/training_utils/train_loop.py:57\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(model, device, config, train_dataloader, val_dataloader, output_path_prefix, existing_model_path, gen_imgs)\u001b[0m\n\u001b[1;32m     53\u001b[0m previous_actions \u001b[38;5;241m=\u001b[39m previous_actions\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     55\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(imgs, previous_frames, previous_actions)\n\u001b[0;32m---> 57\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     59\u001b[0m training_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/home/monsh/anaconda3/envs/project-johnwick/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/monsh/anaconda3/envs/project-johnwick/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/monsh/anaconda3/envs/project-johnwick/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "experiments = [\n",
    "    # {\n",
    "    #     \"name\": \"Experiment1\",\n",
    "    #     \"file\": \"seq1\",\n",
    "    #     \"seq_length\": 1 \n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"Experiment2\",\n",
    "    #     \"file\": \"seq2\",\n",
    "    #     \"seq_length\": 2 \n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"Experiment3\",\n",
    "    #     \"file\": \"seq3\",\n",
    "    #     \"seq_length\": 4 \n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"Experiment4\",\n",
    "    #     \"file\": \"seq4\",\n",
    "    #     \"seq_length\": 8 \n",
    "    # },\n",
    "    {\n",
    "        \"name\": \"Experiment6\",\n",
    "        \"file\": \"seq5_with_ae\",\n",
    "        \"seq_length\": 8 \n",
    "    },\n",
    "]\n",
    "\n",
    "for experiment in experiments:\n",
    "    print(\"-\" * 100)\n",
    "    print(experiment['name'])\n",
    "\n",
    "    args = {\n",
    "        \"config\": \"snake-diffusion/config/Diffusion.yaml\",\n",
    "        \"model_type\": \"ddpm\",\n",
    "        \"output_prefix\": f\"output/{experiment['file']}\",\n",
    "        \"last_checkpoint\": \"\",\n",
    "        \"gen_val_images\": True,\n",
    "\n",
    "        # \"dataset\": \"data/sequences\",\n",
    "        # \"output_loader\": \"output/sequences_loader.pkl\",\n",
    "    }\n",
    "    options = EasyDict(**args)\n",
    "    with open(options.config, 'r') as f:\n",
    "        config = EasyDict(**yaml.safe_load(f))\n",
    "\n",
    "    config['generation']['context_length'] = experiment['seq_length']\n",
    "    config['generation']['actions_count'] = 18\n",
    "\n",
    "    # seq_train_data = SequencesDataset(\n",
    "    #     dataset=train_dataset,\n",
    "    #     seq_length=experiment['seq_length'],\n",
    "    #     transform=transform_to_tensor\n",
    "    # )\n",
    "\n",
    "    # seq_val_data = SequencesDataset(\n",
    "    #     dataset=valid_dataset,\n",
    "    #     seq_length=experiment['seq_length'],\n",
    "    #     transform=transform_to_tensor\n",
    "    # )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_set,\n",
    "        batch_size=config['training']['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=config['training']['num_workers']\n",
    "    )\n",
    "\n",
    "    val_dataloader = DataLoader(\n",
    "        val_set,\n",
    "        batch_size=config['training']['batch_size'],\n",
    "        num_workers=config['training']['num_workers']\n",
    "    )\n",
    "    training_config = TrainingConfig(**config.training)\n",
    "    generation_config = GenerationConfig(**config.generation)\n",
    "\n",
    "    # generation_config.image_size = 160\n",
    "    # generation_config.context_length = 8\n",
    "    # training_config.batch_size = 2\n",
    "\n",
    "    transform_to_tensor = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((.5,.5,.5), (.5,.5,.5))\n",
    "    ])\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # For Mac OS\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "\n",
    "    model: BaseDiffusionModel\n",
    "    if options.model_type == \"edm\":\n",
    "        config = EDMConfig(**instantiate_from_config(config.edm))\n",
    "        model = EDM.from_config(\n",
    "            config=config,\n",
    "            context_length=generation_config.context_length,\n",
    "            device=device,\n",
    "            model=UNet.from_config(\n",
    "                config=config.unet,\n",
    "                in_channels=generation_config.unet_input_channels,\n",
    "                out_channels=generation_config.output_channels,\n",
    "                actions_count=generation_config.actions_count,\n",
    "                seq_length=generation_config.context_length\n",
    "            )\n",
    "        )\n",
    "    elif options.model_type == \"ddpm\":\n",
    "        config = DDPMConfig(**instantiate_from_config(config.ddpm))\n",
    "        model = DDPM.from_config(\n",
    "            config=config,\n",
    "            context_length=generation_config.context_length,\n",
    "            device=device,\n",
    "            model=UNet.from_config(\n",
    "                config=config.unet,\n",
    "                in_channels=generation_config.unet_input_channels,\n",
    "                out_channels=generation_config.output_channels,\n",
    "                actions_count=generation_config.actions_count,\n",
    "                seq_length=generation_config.context_length,\n",
    "                T=config.T,\n",
    "                # player_autoencoder=config.player_autoencoder,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def gen_val_images(epoch: int):\n",
    "        _generate_and_save_sample_imgs(model, val_set, epoch, device, generation_config.context_length)\n",
    "\n",
    "\n",
    "    print(f\"Start training {options.model_type}\")\n",
    "    train_loop(\n",
    "        model=model,\n",
    "        device=device,\n",
    "        config=training_config,\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloader=val_dataloader,\n",
    "        output_path_prefix=options.output_prefix,\n",
    "        existing_model_path=None, #options[\"last_checkpoint\"],\n",
    "        gen_imgs=gen_val_images if options.gen_val_images else None\n",
    "    )\n",
    "\n",
    "print(\"-\"*100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-johnwick",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
